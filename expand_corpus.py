#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∫–æ—Ä–ø—É—Å–∞ —Å–ª–æ–≤ –∏ —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∫–ª–æ–Ω–µ–Ω–∏–π –∏ —Å–ø—Ä—è–∂–µ–Ω–∏–π
"""

import json
import random
import re
from collections import defaultdict

def load_corpus(filename):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ—Ä–ø—É—Å –∏–∑ JSON —Ñ–∞–π–ª–∞"""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filename}: {e}")
        return None

def save_corpus(corpus, filename):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ—Ä–ø—É—Å –≤ JSON —Ñ–∞–π–ª"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(corpus, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ –ö–æ—Ä–ø—É—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filename}")
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {filename}: {e}")
        return False

def determine_declension_correct(word, features):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Å–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–ª—è —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ"""
    word_lower = word.lower()
    
    # –†–∞–∑–Ω–æ—Å–∫–ª–æ–Ω—è–µ–º—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ (–Ω–∞ -–º—è)
    heteroclitic_words = [
        '–≤—Ä–µ–º—è', '–∏–º—è', '–ø–ª–µ–º—è', '–∑–Ω–∞–º—è', '–ø–ª–∞–º—è', '—Å—Ç—Ä–µ–º—è', '—Ç–µ–º—è', 
        '—Å–µ–º—è', '–±—Ä–µ–º—è', '–≤—ã–º—è', '–ø–ª–µ–º—è', '–≤—Ä–µ–º—è', '–∏–º—è', '–∑–Ω–∞–º—è'
    ]
    if word_lower in heteroclitic_words:
        return 'heteroclitic'
    
    # –ù–µ—Å–∫–ª–æ–Ω—è–µ–º—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
    indeclinable_words = [
        '–∫–æ—Ñ–µ', '–ø–∞–ª—å—Ç–æ', '–∫–∏–Ω–æ', '—Ç–∞–∫—Å–∏', '–º–µ—Ç—Ä–æ', '–∫–∞—Ñ–µ', '–º–µ–Ω—é', 
        '–∞–ª–æ—ç', '–∫–∞–∫–∞–æ', '—Ä–∞–¥–∏–æ', '—à–æ—É', '–∫–∞–∑–∏–Ω–æ', '–∫–∞–±–∞—Ä–µ', '–±—é—Ä–æ', 
        '–¥–µ–ø–æ', '—Ñ–æ–π–µ', '–∞—Ç–µ–ª—å–µ', '–ø–µ–Ω—Å–Ω–µ', '–∫–æ–ª—å–µ', '–∫–∞–∫–∞–¥—É', '–∫–µ–Ω–≥—É—Ä—É',
        '—à–∏–º–ø–∞–Ω–∑–µ', '–∫–∞–∫–∞–æ', '—Ä–∞–¥–∏–æ', '–º–µ—Ç—Ä–æ', '–∫–∏–Ω–æ', '—Ç–∞–∫—Å–∏'
    ]
    if word_lower in indeclinable_words:
        return 'indeclinable'
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –æ–∫–æ–Ω—á–∞–Ω–∏—è–º
    if word_lower.endswith('–∞') or word_lower.endswith('—è'):
        return '1st'
    elif word_lower.endswith('–æ') or word_lower.endswith('–µ'):
        return '2nd'
    elif word_lower.endswith('—å'):
        return '3rd'
    else:
        # –ú—É–∂—Å–∫–æ–π —Ä–æ–¥ –±–µ–∑ –æ–∫–æ–Ω—á–∞–Ω–∏—è - –æ–±—ã—á–Ω–æ 2-–µ —Å–∫–ª–æ–Ω–µ–Ω–∏–µ
        if features.get('gender') == 'MASCULINE':
            return '2nd'
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 1-–µ —Å–∫–ª–æ–Ω–µ–Ω–∏–µ
        return '1st'

def determine_conjugation_correct(word, features):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Å–ø—Ä—è–∂–µ–Ω–∏–µ –¥–ª—è –≥–ª–∞–≥–æ–ª–∞"""
    word_lower = word.lower()
    
    # –ò—Å–∫–ª—é—á–µ–Ω–∏—è –¥–ª—è 1-–≥–æ —Å–ø—Ä—è–∂–µ–Ω–∏—è
    first_conjugation_exceptions = ['–±—Ä–∏—Ç—å', '—Å—Ç–µ–ª–∏—Ç—å', '–∑–∏–∂–¥–∏—Ç—å—Å—è']
    if word_lower in first_conjugation_exceptions:
        return '1st'
    
    # 2-–µ —Å–ø—Ä—è–∂–µ–Ω–∏–µ –ø–æ –æ–∫–æ–Ω—á–∞–Ω–∏—è–º
    second_conjugation_endings = ['–∏—Ç—å', '–∞—Ç—å', '—è—Ç—å', '–µ—Ç—å', '—É—Ç—å', '–æ—Ç—å']
    for ending in second_conjugation_endings:
        if word_lower.endswith(ending):
            return '2nd'
    
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 1-–µ —Å–ø—Ä—è–∂–µ–Ω–∏–µ
    return '1st'

def fix_word_features(word, features):
    """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–ª–æ–≤–∞"""
    fixed_features = features.copy()
    
    if features.get('pos') == 'NOUN':
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Å–∫–ª–æ–Ω–µ–Ω–∏–µ
        correct_declension = determine_declension_correct(word, features)
        fixed_features['declension'] = correct_declension
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–æ–¥ –¥–ª—è –Ω–µ—Å–∫–ª–æ–Ω—è–µ–º—ã—Ö —Å–ª–æ–≤
        if correct_declension == 'indeclinable':
            if word.lower() in ['–∫–æ—Ñ–µ', '–∫–∞–∫–∞–æ', '—Ä–∞–¥–∏–æ', '–º–µ—Ç—Ä–æ', '–∫–∏–Ω–æ', '—Ç–∞–∫—Å–∏']:
                fixed_features['gender'] = 'MASCULINE'
            elif word.lower() in ['–ø–∞–ª—å—Ç–æ', '–∫–∞—Ñ–µ', '–º–µ–Ω—é', '–∞–ª–æ—ç', '–∫–æ–ª—å–µ']:
                fixed_features['gender'] = 'NEUTER'
    
    elif features.get('pos') == 'VERB':
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Å–ø—Ä—è–∂–µ–Ω–∏–µ
        correct_conjugation = determine_conjugation_correct(word, features)
        fixed_features['conjugation'] = correct_conjugation
    
    return fixed_features

def expand_corpus_with_verification():
    """–†–∞—Å—à–∏—Ä—è–µ—Ç –∫–æ—Ä–ø—É—Å –∏ –ø—Ä–æ–≤–æ–¥–∏—Ç —Ç—â–∞—Ç–µ–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É"""
    print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â–∏–π –∫–æ—Ä–ø—É—Å
    current_corpus = load_corpus('opencorpora.json')
    if not current_corpus:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—É—â–∏–π –∫–æ—Ä–ø—É—Å")
        return False
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–æ–ª—å—à–æ–π –∫–æ—Ä–ø—É—Å –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    large_corpus = load_corpus('opencorpora_fixed.json')
    if not large_corpus:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–æ–ª—å—à–æ–π –∫–æ—Ä–ø—É—Å")
        return False
    
    print(f"üìä –¢–µ–∫—É—â–∏–π –∫–æ—Ä–ø—É—Å: {current_corpus['metadata']['total_words']} —Å–ª–æ–≤")
    print(f"üìä –ë–æ–ª—å—à–æ–π –∫–æ—Ä–ø—É—Å: {large_corpus['metadata']['total_words']} —Å–ª–æ–≤")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–ª–æ–≤–∞
    existing_words = set(current_corpus['metadata']['words'].keys())
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–æ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –±–æ–ª—å—à–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞
    new_words = {}
    words_by_type = defaultdict(list)
    
    for word, features in large_corpus['metadata']['words'].items():
        if word not in existing_words:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ —Å–ª–æ–≤–æ –¥–ª—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π
            if (features.get('pos') in ['NOUN', 'VERB', 'ADJECTIVE', 'ADVERB', 'CONJUNCTION'] and
                features.get('case') == 'NOMINATIVE' and 
                features.get('number') == 'SINGULAR'):
                
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                fixed_features = fix_word_features(word, features)
                new_words[word] = fixed_features
                
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç—è–º —Ä–µ—á–∏
                words_by_type[fixed_features.get('pos')].append(word)
    
    print(f"üìù –ù–∞–π–¥–µ–Ω–æ {len(new_words)} –Ω–æ–≤—ã—Ö –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Å–ª–æ–≤")
    print(f"   - –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ: {len(words_by_type['NOUN'])}")
    print(f"   - –ì–ª–∞–≥–æ–ª—ã: {len(words_by_type['VERB'])}")
    print(f"   - –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ: {len(words_by_type['ADJECTIVE'])}")
    print(f"   - –ù–∞—Ä–µ—á–∏—è: {len(words_by_type['ADVERB'])}")
    print(f"   - –°–æ—é–∑—ã: {len(words_by_type['CONJUNCTION'])}")
    
    # –í—ã–±–∏—Ä–∞–µ–º 4000 –ª—É—á—à–∏—Ö —Å–ª–æ–≤
    target_count = 4000
    selected_words = {}
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –≥–ª–∞–≥–æ–ª—ã –¥–ª—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π
    priority_order = ['NOUN', 'VERB', 'ADJECTIVE', 'ADVERB', 'CONJUNCTION']
    
    for pos in priority_order:
        if len(selected_words) >= target_count:
            break
        
        words_of_type = words_by_type[pos]
        random.shuffle(words_of_type)  # –°–ª—É—á–∞–π–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫
        
        for word in words_of_type:
            if len(selected_words) >= target_count:
                break
            selected_words[word] = new_words[word]
    
    print(f"‚úÖ –í—ã–±—Ä–∞–Ω–æ {len(selected_words)} —Å–ª–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –∫–æ—Ä–ø—É—Å
    current_corpus['metadata']['words'].update(selected_words)
    current_corpus['metadata']['total_words'] = len(current_corpus['metadata']['words'])
    current_corpus['metadata']['source'] = 'OpenCorpora + Extended'
    current_corpus['metadata']['revision'] = f"extended_{current_corpus['metadata']['total_words']}"
    
    # –ü—Ä–æ–≤–æ–¥–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –≤—Å–µ—Ö —Å–ª–æ–≤
    print("üîç –ü—Ä–æ–≤–æ–¥–∏–º —Ç—â–∞—Ç–µ–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –≤—Å–µ—Ö —Å–ª–æ–≤...")
    
    verification_errors = []
    declension_stats = defaultdict(int)
    conjugation_stats = defaultdict(int)
    
    for word, features in current_corpus['metadata']['words'].items():
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∫–ª–æ–Ω–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö
        if features.get('pos') == 'NOUN':
            correct_declension = determine_declension_correct(word, features)
            if features.get('declension') != correct_declension:
                verification_errors.append(f"–°–∫–ª–æ–Ω–µ–Ω–∏–µ '{word}': {features.get('declension')} ‚Üí {correct_declension}")
                features['declension'] = correct_declension
            declension_stats[correct_declension] += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø—Ä—è–∂–µ–Ω–∏—è –≥–ª–∞–≥–æ–ª–æ–≤
        elif features.get('pos') == 'VERB':
            correct_conjugation = determine_conjugation_correct(word, features)
            if features.get('conjugation') != correct_conjugation:
                verification_errors.append(f"–°–ø—Ä—è–∂–µ–Ω–∏–µ '{word}': {features.get('conjugation')} ‚Üí {correct_conjugation}")
                features['conjugation'] = correct_conjugation
            conjugation_stats[correct_conjugation] += 1
    
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∫–ª–æ–Ω–µ–Ω–∏–π:")
    for decl, count in declension_stats.items():
        print(f"   - {decl}: {count} —Å–ª–æ–≤")
    
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–ø—Ä—è–∂–µ–Ω–∏–π:")
    for conj, count in conjugation_stats.items():
        print(f"   - {conj}: {count} —Å–ª–æ–≤")
    
    if verification_errors:
        print(f"‚ö†Ô∏è –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ {len(verification_errors)} –æ—à–∏–±–æ–∫:")
        for error in verification_errors[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
            print(f"   {error}")
        if len(verification_errors) > 10:
            print(f"   ... –∏ –µ—â–µ {len(verification_errors) - 10} –æ—à–∏–±–æ–∫")
    else:
        print("‚úÖ –í—Å–µ —Å–∫–ª–æ–Ω–µ–Ω–∏—è –∏ —Å–ø—Ä—è–∂–µ–Ω–∏—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã!")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å
    if save_corpus(current_corpus, 'opencorpora_extended.json'):
        print(f"üéâ –ö–æ—Ä–ø—É—Å —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å—à–∏—Ä–µ–Ω –¥–æ {current_corpus['metadata']['total_words']} —Å–ª–æ–≤!")
        return True
    
    return False

if __name__ == "__main__":
    success = expand_corpus_with_verification()
    if success:
        print("\n‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
    else:
        print("\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏ –∫–æ—Ä–ø—É—Å–∞")
